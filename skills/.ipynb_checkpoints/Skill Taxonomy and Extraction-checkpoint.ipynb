{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skill Taxonomy Builder with Embeddings (Improved Extraction)\n",
    "\n",
    "This notebook helps you:\n",
    "1. Build a taxonomy structure from raw skills\n",
    "2. Generate high-quality variations and abbreviations (with improved filtering)\n",
    "3. Compute and store embeddings efficiently\n",
    "4. Set up NumPy-based similarity search with deduplication and scoring improvements\n",
    "\n",
    "## Prerequisites\n",
    "```bash\n",
    "pip install sentence-transformers pandas numpy scikit-learn rapidfuzz pyarrow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from rapidfuzz import fuzz, process\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import re\n",
    "from pathlib import Path\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Build Taxonomy Structure\n",
    "\n",
    "We'll analyze your 35,000 skills to create a hierarchical taxonomy using:\n",
    "- Pattern matching for common categories\n",
    "- Hierarchical clustering based on semantic similarity\n",
    "- Parent-child relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your skills file\n",
    "# Adjust the path and format as needed\n",
    "def load_skills(filepath):\n",
    "    \"\"\"\n",
    "    Load skills from a text file (one skill per line)\n",
    "    Returns a pandas DataFrame\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        skills = [line.strip() for line in f if line.strip()]\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'skill_id': [f'SKILL_{i:05d}' for i in range(len(skills))],\n",
    "        'canonical_name': skills,\n",
    "        'normalized_name': [s.lower().strip() for s in skills]\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load your skills\n",
    "skills_df = load_skills('your_skills_file.txt')  # Update this path\n",
    "print(f\"Loaded {len(skills_df)} skills\")\n",
    "skills_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic taxonomy building using keyword patterns\n",
    "def extract_category_from_patterns(skill_name):\n",
    "    \"\"\"\n",
    "    Extract likely category based on common patterns\n",
    "    Customize these patterns based on your domain\n",
    "    \"\"\"\n",
    "    skill_lower = skill_name.lower()\n",
    "    \n",
    "    # Programming languages\n",
    "    prog_langs = ['python', 'java', 'javascript', 'c++', 'ruby', 'go', 'rust', 'php', 'swift']\n",
    "    if any(lang in skill_lower for lang in prog_langs):\n",
    "        return 'Programming Languages'\n",
    "    \n",
    "    # Data Science & ML\n",
    "    ds_keywords = ['machine learning', 'data science', 'deep learning', 'neural network', \n",
    "                   'tensorflow', 'pytorch', 'scikit-learn', 'nlp', 'computer vision']\n",
    "    if any(kw in skill_lower for kw in ds_keywords):\n",
    "        return 'Data Science & AI'\n",
    "    \n",
    "    # Cloud & DevOps\n",
    "    cloud_keywords = ['aws', 'azure', 'gcp', 'docker', 'kubernetes', 'terraform', 'ci/cd', 'devops']\n",
    "    if any(kw in skill_lower for kw in cloud_keywords):\n",
    "        return 'Cloud & DevOps'\n",
    "    \n",
    "    # Databases\n",
    "    db_keywords = ['sql', 'database', 'postgresql', 'mongodb', 'redis', 'mysql', 'oracle']\n",
    "    if any(kw in skill_lower for kw in db_keywords):\n",
    "        return 'Databases'\n",
    "    \n",
    "    # Web Development\n",
    "    web_keywords = ['html', 'css', 'react', 'angular', 'vue', 'frontend', 'backend', 'web development']\n",
    "    if any(kw in skill_lower for kw in web_keywords):\n",
    "        return 'Web Development'\n",
    "    \n",
    "    # Add more categories as needed\n",
    "    \n",
    "    return 'General'\n",
    "\n",
    "# Apply pattern-based categorization\n",
    "skills_df['category'] = skills_df['canonical_name'].apply(extract_category_from_patterns)\n",
    "\n",
    "print(\"\\nCategory distribution:\")\n",
    "print(skills_df['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect parent-child relationships\n",
    "def find_parent_skills(skill, all_skills, threshold=0.85):\n",
    "    \"\"\"\n",
    "    Find potential parent skills (broader skills that contain this one)\n",
    "    Example: 'Python Programming' is parent of 'Python Django'\n",
    "    \"\"\"\n",
    "    skill_lower = skill.lower()\n",
    "    parents = []\n",
    "    \n",
    "    for other_skill in all_skills:\n",
    "        if skill == other_skill:\n",
    "            continue\n",
    "            \n",
    "        other_lower = other_skill.lower()\n",
    "        \n",
    "        # Check if skill contains the other (other is more general)\n",
    "        if other_lower in skill_lower and other_lower != skill_lower:\n",
    "            # Check token overlap to avoid false positives\n",
    "            skill_tokens = set(skill_lower.split())\n",
    "            other_tokens = set(other_lower.split())\n",
    "            \n",
    "            if other_tokens.issubset(skill_tokens):\n",
    "                parents.append(other_skill)\n",
    "    \n",
    "    return parents\n",
    "\n",
    "# Find parent relationships (this can take a while for 35k skills)\n",
    "print(\"Finding parent-child relationships...\")\n",
    "all_skill_names = skills_df['canonical_name'].tolist()\n",
    "skills_df['parent_skills'] = skills_df['canonical_name'].apply(\n",
    "    lambda x: find_parent_skills(x, all_skill_names)\n",
    ")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\nSkills with parents:\")\n",
    "skills_with_parents = skills_df[skills_df['parent_skills'].apply(len) > 0]\n",
    "print(f\"Found {len(skills_with_parents)} skills with parent relationships\")\n",
    "skills_with_parents[['canonical_name', 'parent_skills']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Generate High-Quality Variations and Abbreviations\n",
    "\n",
    "We'll use a data-driven approach with improved filtering:\n",
    "- Rule-based abbreviation generation\n",
    "- Common typo patterns\n",
    "- Case variations (filtered to avoid false positives)\n",
    "- Token reordering for multi-word skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_abbreviations(skill_name):\n",
    "    \"\"\"\n",
    "    Generate likely abbreviations using rules\n",
    "    \"\"\"\n",
    "    abbreviations = set()\n",
    "    \n",
    "    # Remove common words that are typically not abbreviated\n",
    "    stopwords = {'and', 'or', 'the', 'of', 'for', 'with', 'in', 'on', 'at'}\n",
    "    \n",
    "    tokens = skill_name.split()\n",
    "    filtered_tokens = [t for t in tokens if t.lower() not in stopwords]\n",
    "    \n",
    "    if len(filtered_tokens) > 1:\n",
    "        # First letter of each word\n",
    "        abbr = ''.join([t[0].upper() for t in filtered_tokens])\n",
    "        abbreviations.add(abbr)\n",
    "        \n",
    "        # First letter lowercase version\n",
    "        abbreviations.add(abbr.lower())\n",
    "        \n",
    "        # Common pattern: First word + first letter of others\n",
    "        if len(filtered_tokens) >= 2:\n",
    "            first_word = filtered_tokens[0]\n",
    "            rest_abbr = ''.join([t[0].upper() for t in filtered_tokens[1:]])\n",
    "            abbreviations.add(f\"{first_word}{rest_abbr}\")\n",
    "    \n",
    "    # Known common abbreviations (add your domain-specific ones)\n",
    "    known_abbrevs = {\n",
    "        'machine learning': ['ML', 'ml'],\n",
    "        'artificial intelligence': ['AI', 'ai'],\n",
    "        'natural language processing': ['NLP', 'nlp'],\n",
    "        'computer vision': ['CV', 'cv'],\n",
    "        'deep learning': ['DL', 'dl'],\n",
    "        'data science': ['DS', 'ds'],\n",
    "        'application programming interface': ['API', 'api'],\n",
    "        'structured query language': ['SQL', 'sql'],\n",
    "        'continuous integration': ['CI', 'ci'],\n",
    "        'continuous deployment': ['CD', 'cd'],\n",
    "    }\n",
    "    \n",
    "    skill_lower = skill_name.lower()\n",
    "    for phrase, abbrevs in known_abbrevs.items():\n",
    "        if phrase in skill_lower:\n",
    "            abbreviations.update(abbrevs)\n",
    "    \n",
    "    return list(abbreviations)\n",
    "\n",
    "def generate_common_typos(skill_name):\n",
    "    \"\"\"\n",
    "    Generate common typo patterns\n",
    "    \"\"\"\n",
    "    typos = set()\n",
    "    skill_lower = skill_name.lower()\n",
    "    \n",
    "    # Common character swaps\n",
    "    swaps = [('ie', 'ei'), ('ph', 'f'), ('tion', 'sion')]\n",
    "    for old, new in swaps:\n",
    "        if old in skill_lower:\n",
    "            typos.add(skill_lower.replace(old, new))\n",
    "    \n",
    "    # Double letter removals (programming -> programing)\n",
    "    for i in range(len(skill_lower) - 1):\n",
    "        if skill_lower[i] == skill_lower[i+1]:\n",
    "            typo = skill_lower[:i] + skill_lower[i+1:]\n",
    "            typos.add(typo)\n",
    "    \n",
    "    return list(typos)\n",
    "\n",
    "def generate_variations(skill_name):\n",
    "    \"\"\"\n",
    "    Generate all variations of a skill with improved filtering\n",
    "    \"\"\"\n",
    "    variations = set()\n",
    "    tokens = skill_name.split()\n",
    "    \n",
    "    # For single-word skills, be more conservative with variations\n",
    "    if len(tokens) == 1:\n",
    "        variations.add(skill_name.lower())\n",
    "        variations.add(skill_name.upper())\n",
    "        # Only add abbreviations for longer technical terms\n",
    "        if len(skill_name) > 4:\n",
    "            variations.update(generate_abbreviations(skill_name))\n",
    "        variations.discard(skill_name)\n",
    "        return list(variations)\n",
    "    \n",
    "    # For multi-word skills, generate full variations\n",
    "    variations.add(skill_name)\n",
    "    variations.add(skill_name.lower())\n",
    "    variations.add(skill_name.upper())\n",
    "    variations.add(skill_name.title())\n",
    "    \n",
    "    # Abbreviations\n",
    "    variations.update(generate_abbreviations(skill_name))\n",
    "    \n",
    "    # Common typos (limit to avoid explosion)\n",
    "    typos = generate_common_typos(skill_name)\n",
    "    variations.update(typos[:5])\n",
    "    \n",
    "    # Token reordering for 2-word skills\n",
    "    if len(tokens) == 2:\n",
    "        variations.add(f\"{tokens[1]} {tokens[0]}\")\n",
    "    \n",
    "    # Hyphen/underscore variations\n",
    "    if ' ' in skill_name:\n",
    "        variations.add(skill_name.replace(' ', '-'))\n",
    "        variations.add(skill_name.replace(' ', '_'))\n",
    "    \n",
    "    # Remove the original to avoid duplication\n",
    "    variations.discard(skill_name)\n",
    "    \n",
    "    # Filter out problematic variations (common words that cause false positives)\n",
    "    common_words = {'project', 'projects', 'management', 'analysis', 'development', \n",
    "                    'design', 'testing', 'planning', 'support', 'systems', 'data',\n",
    "                    'business', 'technical', 'customer', 'service', 'process'}\n",
    "    \n",
    "    filtered_variations = []\n",
    "    for var in variations:\n",
    "        var_lower = var.lower()\n",
    "        # Keep variations that:\n",
    "        # 1. Are not single common words, OR\n",
    "        # 2. Have special characters (hyphens, underscores)\n",
    "        if var_lower not in common_words or ' ' in var or '-' in var or '_' in var:\n",
    "            filtered_variations.append(var)\n",
    "    \n",
    "    return filtered_variations\n",
    "\n",
    "# Generate variations for all skills\n",
    "print(\"Generating variations...\")\n",
    "skills_df['variations'] = skills_df['canonical_name'].apply(generate_variations)\n",
    "\n",
    "# Show statistics\n",
    "avg_variations = skills_df['variations'].apply(len).mean()\n",
    "total_variations = skills_df['variations'].apply(len).sum()\n",
    "print(f\"\\nGenerated {total_variations:,} total variations\")\n",
    "print(f\"Average {avg_variations:.1f} variations per skill\")\n",
    "\n",
    "# Show examples\n",
    "print(\"\\nExample variations:\")\n",
    "for idx in skills_df.sample(min(5, len(skills_df))).index:\n",
    "    skill = skills_df.loc[idx, 'canonical_name']\n",
    "    vars = skills_df.loc[idx, 'variations']\n",
    "    print(f\"\\n{skill}:\")\n",
    "    print(f\"  {vars[:10]}\")  # Show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Compute and Store Embeddings\n",
    "\n",
    "We'll use sentence-transformers to generate embeddings for:\n",
    "- Canonical skill names\n",
    "- All variations\n",
    "\n",
    "These will be pre-computed and stored for fast loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "# Options:\n",
    "# - 'all-MiniLM-L6-v2': Fast, good balance (384 dimensions)\n",
    "# - 'multi-qa-MiniLM-L6-cos-v1': Better for asymmetric search\n",
    "# - 'all-mpnet-base-v2': Higher quality, slower (768 dimensions)\n",
    "\n",
    "print(\"Loading embedding model...\")\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embedding_dim = model.get_sentence_embedding_dimension()\n",
    "print(f\"Model loaded. Embedding dimension: {embedding_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings for canonical names\n",
    "print(\"Computing embeddings for canonical skill names...\")\n",
    "canonical_names = skills_df['canonical_name'].tolist()\n",
    "canonical_embeddings = model.encode(\n",
    "    canonical_names,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "# Store embeddings in dataframe\n",
    "skills_df['embedding'] = list(canonical_embeddings)\n",
    "\n",
    "print(f\"Computed {len(canonical_embeddings)} embeddings\")\n",
    "print(f\"Embedding shape: {canonical_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variation-to-skill mapping with embeddings\n",
    "print(\"\\nComputing embeddings for all variations...\")\n",
    "\n",
    "variation_data = []\n",
    "for idx, row in skills_df.iterrows():\n",
    "    skill_id = row['skill_id']\n",
    "    canonical = row['canonical_name']\n",
    "    \n",
    "    # Add canonical name\n",
    "    variation_data.append({\n",
    "        'skill_id': skill_id,\n",
    "        'canonical_name': canonical,\n",
    "        'variation': canonical,\n",
    "        'is_canonical': True\n",
    "    })\n",
    "    \n",
    "    # Add all variations\n",
    "    for var in row['variations']:\n",
    "        variation_data.append({\n",
    "            'skill_id': skill_id,\n",
    "            'canonical_name': canonical,\n",
    "            'variation': var,\n",
    "            'is_canonical': False\n",
    "        })\n",
    "\n",
    "variations_df = pd.DataFrame(variation_data)\n",
    "print(f\"Total entries (canonical + variations): {len(variations_df):,}\")\n",
    "\n",
    "# Compute embeddings for all variations\n",
    "all_variations = variations_df['variation'].tolist()\n",
    "variation_embeddings = model.encode(\n",
    "    all_variations,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "variations_df['embedding'] = list(variation_embeddings)\n",
    "print(f\"\\nComputed {len(variation_embeddings):,} variation embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the complete taxonomy with embeddings\n",
    "print(\"Saving taxonomy...\")\n",
    "\n",
    "# Save as Parquet (preserves embeddings)\n",
    "skills_df.to_parquet('skill_taxonomy.parquet', index=False)\n",
    "variations_df.to_parquet('skill_variations.parquet', index=False)\n",
    "\n",
    "# Also save as JSON for human readability (without embeddings)\n",
    "skills_json = []\n",
    "for idx, row in skills_df.iterrows():\n",
    "    skills_json.append({\n",
    "        'skill_id': row['skill_id'],\n",
    "        'canonical_name': row['canonical_name'],\n",
    "        'category': row['category'],\n",
    "        'parent_skills': row['parent_skills'],\n",
    "        'variations': row['variations'],\n",
    "        # Embeddings excluded from JSON to keep file size reasonable\n",
    "    })\n",
    "\n",
    "with open('skill_taxonomy.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(skills_json, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✓ Saved skill_taxonomy.parquet (with embeddings)\")\n",
    "print(\"✓ Saved skill_variations.parquet (with embeddings)\")\n",
    "print(\"✓ Saved skill_taxonomy.json (human-readable, no embeddings)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prepare NumPy-based Similarity Search\n",
    "\n",
    "Instead of FAISS (which can cause segmentation faults), we'll use NumPy for similarity search.\n",
    "This is stable, portable, and still very fast for your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize and save embeddings for similarity search\n",
    "print(\"Preparing embeddings for similarity search...\")\n",
    "\n",
    "# Normalize canonical embeddings\n",
    "canonical_embeddings_normalized = canonical_embeddings.astype('float32')\n",
    "canonical_embeddings_normalized = canonical_embeddings_normalized / np.linalg.norm(\n",
    "    canonical_embeddings_normalized, axis=1, keepdims=True\n",
    ")\n",
    "\n",
    "# Save normalized canonical embeddings\n",
    "np.save('skill_canonical_embeddings.npy', canonical_embeddings_normalized)\n",
    "print(f\"✓ Saved {len(canonical_embeddings_normalized)} canonical embeddings\")\n",
    "\n",
    "# Normalize variation embeddings\n",
    "variation_embeddings_normalized = variation_embeddings.astype('float32')\n",
    "variation_embeddings_normalized = variation_embeddings_normalized / np.linalg.norm(\n",
    "    variation_embeddings_normalized, axis=1, keepdims=True\n",
    ")\n",
    "\n",
    "# Save normalized variation embeddings\n",
    "np.save('skill_variations_embeddings.npy', variation_embeddings_normalized)\n",
    "print(f\"✓ Saved {len(variation_embeddings_normalized)} variation embeddings\")\n",
    "\n",
    "print(\"\\n✓ All embeddings saved and ready for fast similarity search!\")\n",
    "\n",
    "# Cleanup to free memory\n",
    "del canonical_embeddings_normalized\n",
    "del variation_embeddings_normalized\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runtime Usage: Improved SkillExtractor Class\n",
    "\n",
    "This version includes:\n",
    "- Length-based scoring penalty (prevents partial matches from scoring too high)\n",
    "- Similar skill deduplication (removes redundant results)\n",
    "- Better handling of edge cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkillExtractor:\n",
    "    \"\"\"\n",
    "    Fast skill extraction using pre-built taxonomy and numpy similarity search\n",
    "    with improved scoring and deduplication\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 taxonomy_path='skill_taxonomy.parquet',\n",
    "                 variations_path='skill_variations.parquet',\n",
    "                 canonical_embeddings_path='skill_canonical_embeddings.npy',\n",
    "                 variations_embeddings_path='skill_variations_embeddings.npy',\n",
    "                 model_name='all-MiniLM-L6-v2'):\n",
    "        \n",
    "        print(\"Loading skill extractor...\")\n",
    "        \n",
    "        # Load taxonomy\n",
    "        self.skills_df = pd.read_parquet(taxonomy_path)\n",
    "        self.variations_df = pd.read_parquet(variations_path)\n",
    "        \n",
    "        # Load embeddings (numpy arrays)\n",
    "        self.canonical_embeddings = np.load(canonical_embeddings_path)\n",
    "        self.variations_embeddings = np.load(variations_embeddings_path)\n",
    "        \n",
    "        # Load embedding model\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "        print(f\"✓ Loaded {len(self.skills_df)} skills\")\n",
    "        print(f\"✓ Loaded {len(self.variations_df)} variations\")\n",
    "        print(f\"✓ Ready for extraction\")\n",
    "    \n",
    "    def extract_from_text(self, text, threshold=0.5, top_k=10, use_variations=True,\n",
    "                         deduplicate_similar=True, dedup_threshold=0.85,\n",
    "                         apply_length_penalty=True):\n",
    "        \"\"\"\n",
    "        Extract skills from text using numpy-based similarity search\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to extract skills from\n",
    "            threshold: Minimum similarity threshold (0-1)\n",
    "            top_k: Number of top matches to consider per n-gram\n",
    "            use_variations: Whether to search variations or just canonical names\n",
    "            deduplicate_similar: Remove skills that are very similar to each other\n",
    "            dedup_threshold: Similarity threshold for considering skills duplicates\n",
    "            apply_length_penalty: Penalize matches where ngram is much shorter than skill\n",
    "        \n",
    "        Returns:\n",
    "            List of detected skills with similarity scores\n",
    "        \"\"\"\n",
    "        # Generate n-grams from text\n",
    "        ngrams = self._generate_ngrams(text, max_n=5)\n",
    "        \n",
    "        if not ngrams:\n",
    "            return []\n",
    "        \n",
    "        # Encode n-grams\n",
    "        ngram_embeddings = self.model.encode(ngrams, convert_to_numpy=True)\n",
    "        ngram_embeddings = ngram_embeddings / np.linalg.norm(\n",
    "            ngram_embeddings, axis=1, keepdims=True\n",
    "        )\n",
    "        \n",
    "        # Search for matches\n",
    "        detected_skills = {}\n",
    "        \n",
    "        for i, ngram in enumerate(ngrams):\n",
    "            query = ngram_embeddings[i:i+1]\n",
    "            \n",
    "            if use_variations:\n",
    "                # Compute cosine similarity with all variations (dot product)\n",
    "                similarities = np.dot(self.variations_embeddings, query.T).flatten()\n",
    "                \n",
    "                # Get top k indices\n",
    "                if len(similarities) > top_k:\n",
    "                    top_indices = np.argpartition(similarities, -top_k)[-top_k:]\n",
    "                    top_indices = top_indices[np.argsort(similarities[top_indices])][::-1]\n",
    "                else:\n",
    "                    top_indices = np.argsort(similarities)[::-1]\n",
    "                \n",
    "                for idx in top_indices:\n",
    "                    dist = similarities[idx]\n",
    "                    if dist >= threshold:\n",
    "                        match = self.variations_df.iloc[idx]\n",
    "                        skill_id = match['skill_id']\n",
    "                        canonical = match['canonical_name']\n",
    "                        matched_variation = match['variation']\n",
    "                        \n",
    "                        # Apply length penalty\n",
    "                        if apply_length_penalty:\n",
    "                            adjusted_score = self._apply_length_penalty(\n",
    "                                dist, ngram, canonical\n",
    "                            )\n",
    "                        else:\n",
    "                            adjusted_score = dist\n",
    "                        \n",
    "                        # Only update if this is a better match\n",
    "                        if skill_id not in detected_skills or adjusted_score > detected_skills[skill_id]['score']:\n",
    "                            detected_skills[skill_id] = {\n",
    "                                'canonical_name': canonical,\n",
    "                                'matched_text': ngram,\n",
    "                                'matched_variation': matched_variation,\n",
    "                                'score': float(adjusted_score),\n",
    "                                'raw_similarity': float(dist)\n",
    "                            }\n",
    "            else:\n",
    "                # Search canonical embeddings\n",
    "                similarities = np.dot(self.canonical_embeddings, query.T).flatten()\n",
    "                \n",
    "                if len(similarities) > top_k:\n",
    "                    top_indices = np.argpartition(similarities, -top_k)[-top_k:]\n",
    "                    top_indices = top_indices[np.argsort(similarities[top_indices])][::-1]\n",
    "                else:\n",
    "                    top_indices = np.argsort(similarities)[::-1]\n",
    "                \n",
    "                for idx in top_indices:\n",
    "                    dist = similarities[idx]\n",
    "                    if dist >= threshold:\n",
    "                        match = self.skills_df.iloc[idx]\n",
    "                        skill_id = match['skill_id']\n",
    "                        canonical = match['canonical_name']\n",
    "                        \n",
    "                        # Apply length penalty\n",
    "                        if apply_length_penalty:\n",
    "                            adjusted_score = self._apply_length_penalty(\n",
    "                                dist, ngram, canonical\n",
    "                            )\n",
    "                        else:\n",
    "                            adjusted_score = dist\n",
    "                        \n",
    "                        if skill_id not in detected_skills or adjusted_score > detected_skills[skill_id]['score']:\n",
    "                            detected_skills[skill_id] = {\n",
    "                                'canonical_name': canonical,\n",
    "                                'matched_text': ngram,\n",
    "                                'score': float(adjusted_score),\n",
    "                                'raw_similarity': float(dist)\n",
    "                            }\n",
    "        \n",
    "        # Sort by score\n",
    "        results = sorted(detected_skills.values(), key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Deduplicate similar skills\n",
    "        if deduplicate_similar and len(results) > 1:\n",
    "            results = self._deduplicate_similar_skills(results, dedup_threshold)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _apply_length_penalty(self, similarity, ngram, canonical_skill):\n",
    "        \"\"\"\n",
    "        Apply penalty when matched n-gram is much shorter than the skill name\n",
    "        This prevents partial matches from scoring too high\n",
    "        \"\"\"\n",
    "        ngram_len = len(ngram.split())\n",
    "        skill_len = len(canonical_skill.split())\n",
    "        \n",
    "        if ngram_len < skill_len:\n",
    "            # Penalize based on length difference\n",
    "            # E.g., if ngram is 1 word and skill is 3 words, penalty = 1/3\n",
    "            length_penalty = ngram_len / skill_len\n",
    "            # Apply penalty with a minimum floor to avoid over-penalization\n",
    "            penalty_factor = max(0.5, length_penalty)\n",
    "            adjusted_score = similarity * penalty_factor\n",
    "        else:\n",
    "            adjusted_score = similarity\n",
    "        \n",
    "        return adjusted_score\n",
    "    \n",
    "    def _deduplicate_similar_skills(self, results, threshold=0.85):\n",
    "        \"\"\"\n",
    "        Remove redundant skills that are very similar to higher-scoring skills\n",
    "        \"\"\"\n",
    "        if len(results) <= 1:\n",
    "            return results\n",
    "        \n",
    "        # Get embeddings for all detected skills\n",
    "        skill_names = [r['canonical_name'] for r in results]\n",
    "        skill_embeddings = self.model.encode(skill_names, convert_to_numpy=True)\n",
    "        skill_embeddings = skill_embeddings / np.linalg.norm(skill_embeddings, axis=1, keepdims=True)\n",
    "        \n",
    "        # Keep track of which skills to keep\n",
    "        keep_indices = []\n",
    "        \n",
    "        for i in range(len(results)):\n",
    "            # Always keep the first (highest scoring)\n",
    "            if i == 0:\n",
    "                keep_indices.append(i)\n",
    "                continue\n",
    "            \n",
    "            # Check similarity with all higher-scoring kept skills\n",
    "            should_keep = True\n",
    "            for j in keep_indices:\n",
    "                similarity = np.dot(skill_embeddings[i], skill_embeddings[j])\n",
    "                if similarity > threshold:\n",
    "                    should_keep = False\n",
    "                    break\n",
    "            \n",
    "            if should_keep:\n",
    "                keep_indices.append(i)\n",
    "        \n",
    "        return [results[i] for i in keep_indices]\n",
    "    \n",
    "    def _generate_ngrams(self, text, max_n=5):\n",
    "        \"\"\"\n",
    "        Generate n-grams from text (1 to max_n words)\n",
    "        \"\"\"\n",
    "        # Clean and tokenize\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s-]', ' ', text)\n",
    "        tokens = text.lower().split()\n",
    "        \n",
    "        ngrams = []\n",
    "        for n in range(1, min(max_n + 1, len(tokens) + 1)):\n",
    "            for i in range(len(tokens) - n + 1):\n",
    "                ngram = ' '.join(tokens[i:i+n])\n",
    "                ngrams.append(ngram)\n",
    "        \n",
    "        return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "extractor = SkillExtractor()\n",
    "\n",
    "# Test with sample text\n",
    "sample_text = \"\"\"\n",
    "I have 5 years of experience in Python programming and machine learning. \n",
    "I've worked extensively with TensorFlow and PyTorch for deep learning projects.\n",
    "I'm also proficient in SQL databases and cloud platforms like AWS.\n",
    "\"\"\"\n",
    "\n",
    "detected = extractor.extract_from_text(\n",
    "    sample_text, \n",
    "    threshold=0.6, \n",
    "    use_variations=True,\n",
    "    deduplicate_similar=True,\n",
    "    apply_length_penalty=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDetected {len(detected)} skills:\")\n",
    "for skill in detected[:10]:  # Show top 10\n",
    "    print(f\"  • {skill['canonical_name']} (score: {skill['score']:.3f})\")\n",
    "    print(f\"    Matched: '{skill['matched_text']}'\")\n",
    "    if 'matched_variation' in skill:\n",
    "        print(f\"    Via variation: '{skill['matched_variation']}'\")\n",
    "    if 'raw_similarity' in skill:\n",
    "        print(f\"    Raw similarity: {skill['raw_similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Improvements\n",
    "\n",
    "### Key Improvements in this version:\n",
    "\n",
    "1. **Better Variation Generation**\n",
    "   - Conservative approach for single-word skills\n",
    "   - Filters out common words that cause false positives\n",
    "   - Reduces noise from generic terms like \"projects\", \"management\", etc.\n",
    "\n",
    "2. **Length-Based Scoring Penalty**\n",
    "   - Prevents partial matches from scoring too high\n",
    "   - E.g., matching \"projects\" to \"Project Management\" gets penalized\n",
    "   - Configurable via `apply_length_penalty` parameter\n",
    "\n",
    "3. **Similar Skill Deduplication**\n",
    "   - Removes redundant results (e.g., \"Machine Learning\" and \"ML\")\n",
    "   - Keeps the highest-scoring match from each cluster\n",
    "   - Configurable via `deduplicate_similar` and `dedup_threshold`\n",
    "\n",
    "4. **Better Diagnostics**\n",
    "   - Returns both adjusted score and raw similarity\n",
    "   - Shows which variation was matched\n",
    "   - Easier to debug and tune thresholds\n",
    "\n",
    "### Tuning Recommendations:\n",
    "\n",
    "- **For higher precision**: Increase `threshold` (e.g., 0.7) and `dedup_threshold` (e.g., 0.9)\n",
    "- **For higher recall**: Decrease `threshold` (e.g., 0.4) and disable length penalty\n",
    "- **For balanced results**: Use defaults (threshold=0.6, dedup_threshold=0.85, length_penalty=True)\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Test on your actual text data and adjust thresholds\n",
    "2. Add more common words to the filter list if needed\n",
    "3. Consider adding exact/fuzzy matching as a complement to semantic search\n",
    "4. Evaluate on labeled data to measure precision/recall\n",
    "5. Fine-tune the length penalty formula for your specific use case"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
