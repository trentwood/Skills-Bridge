{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Job Architecture System with SOC Titles\n",
    "\n",
    "This notebook builds a comprehensive job architecture system using 18,000+ job titles from the Standard Occupational Classification (SOC) system:\n",
    "1. Process and normalize 18K+ SOC job titles\n",
    "2. Build graph database with hierarchical relationships\n",
    "3. Map skills to job titles\n",
    "4. Generate industry/company-specific architectures\n",
    "5. Web services for normalization, career paths, and skills lookup\n",
    "6. Integration with skill extraction service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q networkx pandas numpy scikit-learn sentence-transformers flask flask-cors requests python-dotenv rapidfuzz tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional, Set\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "\n",
    "# ML and embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rapidfuzz import fuzz, process\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Analyze SOC Titles Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SOC titles\n",
    "soc_df = pd.read_csv('/mnt/user-data/uploads/SOC_titles.csv')\n",
    "\n",
    "# Drop the empty column\n",
    "soc_df = soc_df.drop('Unnamed: 4', axis=1)\n",
    "\n",
    "# Filter out 'not available' normalized titles\n",
    "soc_df = soc_df[soc_df['normalized'] != 'not available'].copy()\n",
    "\n",
    "print(f\"Total job titles: {len(soc_df):,}\")\n",
    "print(f\"Unique SOC5 categories: {soc_df['soc5_title'].nunique()}\")\n",
    "print(f\"Unique normalized titles: {soc_df['normalized'].nunique():,}\")\n",
    "print(f\"\\nSample data:\")\n",
    "soc_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze SOC categories\n",
    "print(\"Top 20 SOC5 Categories by Job Title Count:\")\n",
    "print(soc_df['soc5_title'].value_counts().head(20))\n",
    "\n",
    "# Sample different categories\n",
    "print(\"\\nSample categories:\")\n",
    "sample_categories = [\n",
    "    'Software Developers',\n",
    "    'Data Scientists',\n",
    "    'Product Managers',\n",
    "    'Chief Executives',\n",
    "    'Financial Managers'\n",
    "]\n",
    "\n",
    "for cat in sample_categories:\n",
    "    titles = soc_df[soc_df['soc5_title'].str.contains(cat, case=False, na=False)]\n",
    "    if len(titles) > 0:\n",
    "        print(f\"\\n{cat}:\")\n",
    "        print(titles[['title_name', 'normalized']].head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Level Classification\n",
    "\n",
    "Classify all SOC titles into organizational levels based on title patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobLevelClassifier:\n",
    "    \"\"\"Classify job titles into organizational levels\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Level 9: C-Suite / Executive\n",
    "        self.level_9_patterns = [\n",
    "            r'\\bchief\\s+\\w+\\s+officer\\b',\n",
    "            r'\\bceo\\b', r'\\bcfo\\b', r'\\bcto\\b', r'\\bcoo\\b', r'\\bcmo\\b', r'\\bcpo\\b', r'\\bchro\\b',\n",
    "            r'\\bpresident\\b(?!.*\\bassociate\\b)',\n",
    "            r'\\bexecutive\\s+director\\b',\n",
    "        ]\n",
    "        \n",
    "        # Level 8: Senior VP\n",
    "        self.level_8_patterns = [\n",
    "            r'\\bsenior\\s+vice\\s+president\\b',\n",
    "            r'\\bsvp\\b',\n",
    "            r'\\bexecutive\\s+vice\\s+president\\b',\n",
    "            r'\\bevp\\b',\n",
    "        ]\n",
    "        \n",
    "        # Level 7: VP\n",
    "        self.level_7_patterns = [\n",
    "            r'\\bvice\\s+president\\b(?!.*senior)',\n",
    "            r'\\bvp\\b(?!.*senior)',\n",
    "        ]\n",
    "        \n",
    "        # Level 6: Director\n",
    "        self.level_6_patterns = [\n",
    "            r'\\bdirector\\b(?!.*assistant|.*associate|.*deputy)',\n",
    "            r'\\bhead\\s+of\\b',\n",
    "        ]\n",
    "        \n",
    "        # Level 5: Senior Manager\n",
    "        self.level_5_patterns = [\n",
    "            r'\\bsenior\\s+manager\\b',\n",
    "            r'\\bgroup\\s+manager\\b',\n",
    "            r'\\bprogram\\s+manager\\b(?!.*assistant)',\n",
    "            r'\\bprincipal\\b(?!.*engineer|.*scientist|.*designer)',\n",
    "        ]\n",
    "        \n",
    "        # Level 4: Manager / Principal IC\n",
    "        self.level_4_patterns = [\n",
    "            r'\\bmanager\\b(?!.*assistant|.*senior|.*program)',\n",
    "            r'\\bstaff\\s+(engineer|scientist|designer|analyst)\\b',\n",
    "            r'\\bprincipal\\s+(engineer|scientist|designer|analyst)\\b',\n",
    "            r'\\blead\\s+(engineer|scientist|designer|developer|analyst)\\b',\n",
    "            r'\\bsupervisor\\b',\n",
    "        ]\n",
    "        \n",
    "        # Level 3: Senior IC\n",
    "        self.level_3_patterns = [\n",
    "            r'\\bsenior\\s+(engineer|scientist|designer|developer|analyst|consultant|specialist)\\b',\n",
    "            r'\\bsr\\.?\\s+(engineer|scientist|designer|developer|analyst)\\b',\n",
    "        ]\n",
    "        \n",
    "        # Level 2: Mid-level IC\n",
    "        self.level_2_patterns = [\n",
    "            r'\\b(engineer|scientist|designer|developer|analyst|consultant|specialist)\\b(?!.*senior|.*jr|.*junior|.*assistant)',\n",
    "            r'\\btechnician\\b',\n",
    "            r'\\bcoordinator\\b',\n",
    "        ]\n",
    "        \n",
    "        # Level 1: Junior IC\n",
    "        self.level_1_patterns = [\n",
    "            r'\\bjunior\\b',\n",
    "            r'\\bjr\\.?\\b',\n",
    "            r'\\bassociate\\s+(engineer|scientist|designer|developer|analyst|consultant)\\b',\n",
    "            r'\\bassistant\\b',\n",
    "        ]\n",
    "        \n",
    "        # Level 0: Entry / Intern\n",
    "        self.level_0_patterns = [\n",
    "            r'\\bintern\\b',\n",
    "            r'\\btrainee\\b',\n",
    "            r'\\bentry\\s+level\\b',\n",
    "        ]\n",
    "    \n",
    "    def classify(self, title: str) -> int:\n",
    "        \"\"\"Classify a job title into a level (0-9)\"\"\"\n",
    "        title_lower = title.lower()\n",
    "        \n",
    "        # Check levels from highest to lowest\n",
    "        for level, patterns in [\n",
    "            (9, self.level_9_patterns),\n",
    "            (8, self.level_8_patterns),\n",
    "            (7, self.level_7_patterns),\n",
    "            (6, self.level_6_patterns),\n",
    "            (5, self.level_5_patterns),\n",
    "            (4, self.level_4_patterns),\n",
    "            (3, self.level_3_patterns),\n",
    "            (0, self.level_0_patterns),\n",
    "            (1, self.level_1_patterns),\n",
    "            (2, self.level_2_patterns),\n",
    "        ]:\n",
    "            for pattern in patterns:\n",
    "                if re.search(pattern, title_lower):\n",
    "                    return level\n",
    "        \n",
    "        # Default to level 2 (mid-level IC)\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify all titles\n",
    "classifier = JobLevelClassifier()\n",
    "\n",
    "print(\"Classifying job levels...\")\n",
    "soc_df['level'] = soc_df['normalized'].progress_apply(classifier.classify)\n",
    "\n",
    "print(f\"\\nLevel distribution:\")\n",
    "level_dist = soc_df['level'].value_counts().sort_index()\n",
    "print(level_dist)\n",
    "\n",
    "# Show sample titles per level\n",
    "print(\"\\nSample titles per level:\")\n",
    "for level in sorted(soc_df['level'].unique()):\n",
    "    samples = soc_df[soc_df['level'] == level]['normalized'].drop_duplicates().head(5).tolist()\n",
    "    print(f\"\\nLevel {level}:\")\n",
    "    for s in samples:\n",
    "        print(f\"  - {s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Family Classification\n",
    "\n",
    "Group titles into job families based on SOC categories and keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobFamilyClassifier:\n",
    "    \"\"\"Classify job titles into families\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.family_keywords = {\n",
    "            'Engineering': [\n",
    "                'software', 'engineer', 'developer', 'programmer', 'devops', 'sre',\n",
    "                'site reliability', 'systems', 'infrastructure', 'backend', 'frontend',\n",
    "                'full stack', 'mobile', 'web', 'application', 'platform', 'technical'\n",
    "            ],\n",
    "            'Data': [\n",
    "                'data', 'analytics', 'scientist', 'machine learning', 'ml engineer',\n",
    "                'artificial intelligence', 'ai', 'deep learning', 'nlp', 'computer vision',\n",
    "                'data engineer', 'data analyst', 'business intelligence', 'bi'\n",
    "            ],\n",
    "            'Product': [\n",
    "                'product manager', 'product owner', 'product lead', 'product director',\n",
    "                'product management', 'program manager'\n",
    "            ],\n",
    "            'Design': [\n",
    "                'designer', 'ux', 'ui', 'user experience', 'user interface',\n",
    "                'visual design', 'interaction design', 'product design', 'graphic design'\n",
    "            ],\n",
    "            'Sales': [\n",
    "                'sales', 'account executive', 'account manager', 'business development',\n",
    "                'sales engineer', 'sales director', 'revenue', 'commercial'\n",
    "            ],\n",
    "            'Marketing': [\n",
    "                'marketing', 'brand', 'advertising', 'content', 'social media',\n",
    "                'digital marketing', 'growth', 'demand generation', 'communications'\n",
    "            ],\n",
    "            'HR': [\n",
    "                'human resources', 'hr', 'people', 'talent', 'recruiting', 'recruiter',\n",
    "                'people operations', 'compensation', 'benefits', 'training'\n",
    "            ],\n",
    "            'Finance': [\n",
    "                'finance', 'accounting', 'financial', 'controller', 'treasury',\n",
    "                'fp&a', 'financial planning', 'cfo', 'accountant', 'audit'\n",
    "            ],\n",
    "            'Operations': [\n",
    "                'operations', 'supply chain', 'logistics', 'procurement',\n",
    "                'facilities', 'office manager', 'project manager', 'operations manager'\n",
    "            ],\n",
    "            'Customer Success': [\n",
    "                'customer success', 'customer support', 'customer service',\n",
    "                'account management', 'client services', 'support engineer'\n",
    "            ],\n",
    "            'Legal': [\n",
    "                'legal', 'counsel', 'attorney', 'lawyer', 'compliance', 'regulatory'\n",
    "            ],\n",
    "            'Executive': [\n",
    "                'chief executive', 'ceo', 'president', 'chief operating', 'coo',\n",
    "                'chief financial', 'cfo', 'chief technology', 'cto',\n",
    "                'chief product', 'cpo', 'chief marketing', 'cmo', 'executive director'\n",
    "            ],\n",
    "        }\n",
    "    \n",
    "    def classify(self, title: str, soc_category: str = \"\") -> str:\n",
    "        \"\"\"Classify a job title into a family\"\"\"\n",
    "        title_lower = title.lower()\n",
    "        soc_lower = soc_category.lower()\n",
    "        combined = f\"{title_lower} {soc_lower}\"\n",
    "        \n",
    "        # Score each family\n",
    "        scores = {}\n",
    "        for family, keywords in self.family_keywords.items():\n",
    "            score = sum(1 for keyword in keywords if keyword in combined)\n",
    "            if score > 0:\n",
    "                scores[family] = score\n",
    "        \n",
    "        # Return family with highest score\n",
    "        if scores:\n",
    "            return max(scores.items(), key=lambda x: x[1])[0]\n",
    "        \n",
    "        # Default to Operations\n",
    "        return 'Operations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify families\n",
    "family_classifier = JobFamilyClassifier()\n",
    "\n",
    "print(\"Classifying job families...\")\n",
    "soc_df['family'] = soc_df.progress_apply(\n",
    "    lambda row: family_classifier.classify(row['normalized'], row['soc5_title']), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(f\"\\nFamily distribution:\")\n",
    "family_dist = soc_df['family'].value_counts()\n",
    "print(family_dist)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "family_dist.plot(kind='bar')\n",
    "plt.title('Job Titles by Family')\n",
    "plt.xlabel('Job Family')\n",
    "plt.ylabel('Number of Titles')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Enhanced Job Title Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class JobTitle:\n",
    "    id: str\n",
    "    title: str\n",
    "    level: int\n",
    "    family: str\n",
    "    soc_category: str\n",
    "    alternate_titles: List[str] = None\n",
    "    industry: str = \"General\"\n",
    "    company_size: str = \"All\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.alternate_titles is None:\n",
    "            self.alternate_titles = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create job title objects from SOC data\n",
    "# Group by normalized title to collect variations\n",
    "print(\"Creating job title objects...\")\n",
    "\n",
    "job_titles = []\n",
    "grouped = soc_df.groupby('normalized')\n",
    "\n",
    "for idx, (normalized_title, group) in enumerate(tqdm(grouped, desc=\"Processing titles\")):\n",
    "    # Get the most common level and family\n",
    "    level = group['level'].mode()[0] if len(group['level'].mode()) > 0 else group['level'].iloc[0]\n",
    "    family = group['family'].mode()[0] if len(group['family'].mode()) > 0 else group['family'].iloc[0]\n",
    "    soc_category = group['soc5_title'].iloc[0]\n",
    "    \n",
    "    # Collect alternate titles\n",
    "    alternate_titles = group['title_name'].unique().tolist()\n",
    "    if normalized_title in alternate_titles:\n",
    "        alternate_titles.remove(normalized_title)\n",
    "    \n",
    "    job = JobTitle(\n",
    "        id=f\"job_{idx:05d}\",\n",
    "        title=normalized_title,\n",
    "        level=int(level),\n",
    "        family=family,\n",
    "        soc_category=soc_category,\n",
    "        alternate_titles=alternate_titles[:10]  # Limit to 10 alternates\n",
    "    )\n",
    "    \n",
    "    job_titles.append(job)\n",
    "\n",
    "print(f\"\\nCreated {len(job_titles):,} unique job titles\")\n",
    "print(f\"Average alternates per title: {sum(len(j.alternate_titles) for j in job_titles) / len(job_titles):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show examples\n",
    "print(\"Sample job titles:\")\n",
    "for job in job_titles[100:110]:\n",
    "    print(f\"\\n{job.title}\")\n",
    "    print(f\"  Level: {job.level}, Family: {job.family}\")\n",
    "    print(f\"  SOC: {job.soc_category}\")\n",
    "    if job.alternate_titles:\n",
    "        print(f\"  Alternates: {', '.join(job.alternate_titles[:3])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Job Architecture Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobArchitectureGraph:\n",
    "    \"\"\"Graph database for job titles and their relationships\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.job_lookup = {}  # id -> JobTitle\n",
    "        self.title_to_id = {}  # title -> id (lowercase)\n",
    "        \n",
    "    def add_job(self, job: JobTitle):\n",
    "        \"\"\"Add a job title to the graph\"\"\"\n",
    "        self.graph.add_node(job.id, **asdict(job))\n",
    "        self.job_lookup[job.id] = job\n",
    "        self.title_to_id[job.title.lower()] = job.id\n",
    "        \n",
    "        # Add alternate titles\n",
    "        for alt_title in job.alternate_titles:\n",
    "            self.title_to_id[alt_title.lower()] = job.id\n",
    "    \n",
    "    def add_reporting_relationship(self, reports_to_id: str, reports_from_id: str, \n",
    "                                   relationship_type: str = \"reports_to\"):\n",
    "        \"\"\"Add a hierarchical relationship between jobs\"\"\"\n",
    "        self.graph.add_edge(reports_from_id, reports_to_id, relationship=relationship_type)\n",
    "    \n",
    "    def build_hierarchy(self, max_edges_per_node: int = 10):\n",
    "        \"\"\"Build reporting relationships based on levels and families\"\"\"\n",
    "        print(\"Building hierarchy...\")\n",
    "        \n",
    "        # Group jobs by family and level\n",
    "        family_jobs = defaultdict(lambda: defaultdict(list))\n",
    "        \n",
    "        for job_id, job in self.job_lookup.items():\n",
    "            family_jobs[job.family][job.level].append(job_id)\n",
    "        \n",
    "        edges_added = 0\n",
    "        \n",
    "        # Create reporting relationships within each family\n",
    "        for family, levels in tqdm(family_jobs.items(), desc=\"Building family hierarchies\"):\n",
    "            sorted_levels = sorted(levels.keys())\n",
    "            \n",
    "            for i in range(len(sorted_levels) - 1):\n",
    "                current_level = sorted_levels[i]\n",
    "                next_level = sorted_levels[i + 1]\n",
    "                \n",
    "                current_jobs = levels[current_level]\n",
    "                manager_jobs = levels[next_level]\n",
    "                \n",
    "                # Limit edges to avoid graph explosion\n",
    "                for job_id in current_jobs[:max_edges_per_node]:\n",
    "                    for manager_id in manager_jobs[:max_edges_per_node]:\n",
    "                        self.add_reporting_relationship(manager_id, job_id)\n",
    "                        edges_added += 1\n",
    "        \n",
    "        print(f\"Added {edges_added:,} reporting relationships\")\n",
    "    \n",
    "    def get_career_path(self, job_id: str, direction: str = \"up\", limit: int = 20) -> List[JobTitle]:\n",
    "        \"\"\"Get career path from a job\"\"\"\n",
    "        job = self.job_lookup.get(job_id)\n",
    "        if not job:\n",
    "            return []\n",
    "        \n",
    "        if direction == \"up\":\n",
    "            # Higher levels in same family\n",
    "            results = [j for j in self.job_lookup.values() \n",
    "                      if j.family == job.family and j.level > job.level]\n",
    "        elif direction == \"down\":\n",
    "            # Lower levels in same family\n",
    "            results = [j for j in self.job_lookup.values() \n",
    "                      if j.family == job.family and j.level < job.level]\n",
    "        elif direction == \"lateral\":\n",
    "            # Same level, different family\n",
    "            results = [j for j in self.job_lookup.values() \n",
    "                      if j.level == job.level and j.family != job.family]\n",
    "        else:\n",
    "            return []\n",
    "        \n",
    "        # Sort by level\n",
    "        results.sort(key=lambda x: x.level, reverse=(direction == \"up\"))\n",
    "        return results[:limit]\n",
    "    \n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Save graph to file\"\"\"\n",
    "        print(f\"Saving graph to {filepath}...\")\n",
    "        data = {\n",
    "            'job_lookup': {k: asdict(v) for k, v in self.job_lookup.items()},\n",
    "            'title_to_id': self.title_to_id\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        print(f\"Saved {len(self.job_lookup):,} jobs and {len(self.title_to_id):,} title mappings\")\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, filepath: str):\n",
    "        \"\"\"Load graph from file\"\"\"\n",
    "        print(f\"Loading graph from {filepath}...\")\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        graph_obj = cls()\n",
    "        graph_obj.job_lookup = {k: JobTitle(**v) for k, v in data['job_lookup'].items()}\n",
    "        graph_obj.title_to_id = data['title_to_id']\n",
    "        \n",
    "        # Rebuild graph structure\n",
    "        for job_id, job in graph_obj.job_lookup.items():\n",
    "            graph_obj.graph.add_node(job_id, **asdict(job))\n",
    "        \n",
    "        print(f\"Loaded {len(graph_obj.job_lookup):,} jobs\")\n",
    "        return graph_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "job_graph = JobArchitectureGraph()\n",
    "\n",
    "print(\"Adding jobs to graph...\")\n",
    "for job in tqdm(job_titles, desc=\"Adding jobs\"):\n",
    "    job_graph.add_job(job)\n",
    "\n",
    "print(f\"\\nGraph contains {len(job_graph.graph.nodes):,} nodes\")\n",
    "print(f\"Title lookup contains {len(job_graph.title_to_id):,} entries\")\n",
    "\n",
    "# Build hierarchy (this can take a while for large graphs)\n",
    "job_graph.build_hierarchy(max_edges_per_node=5)\n",
    "\n",
    "print(f\"\\nFinal graph: {len(job_graph.graph.nodes):,} nodes, {len(job_graph.graph.edges):,} edges\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Title Normalizer with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JobTitleNormalizer:\n",
    "    \"\"\"Normalize job titles using hybrid matching\"\"\"\n",
    "    \n",
    "    def __init__(self, job_graph: JobArchitectureGraph, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.job_graph = job_graph\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        \n",
    "        print(\"Preparing title data...\")\n",
    "        # Prepare all titles for matching\n",
    "        self.all_titles = []\n",
    "        self.title_to_job = {}\n",
    "        \n",
    "        for job in tqdm(job_graph.job_lookup.values(), desc=\"Collecting titles\"):\n",
    "            self.all_titles.append(job.title)\n",
    "            self.title_to_job[job.title] = job\n",
    "            \n",
    "            for alt in job.alternate_titles:\n",
    "                self.all_titles.append(alt)\n",
    "                self.title_to_job[alt] = job\n",
    "        \n",
    "        print(f\"Total searchable titles: {len(self.all_titles):,}\")\n",
    "        \n",
    "        # Pre-compute embeddings\n",
    "        print(\"Computing embeddings (this may take a few minutes)...\")\n",
    "        self.title_embeddings = self.model.encode(\n",
    "            self.all_titles, \n",
    "            show_progress_bar=True,\n",
    "            batch_size=256\n",
    "        )\n",
    "        print(f\"Embeddings shape: {self.title_embeddings.shape}\")\n",
    "    \n",
    "    def normalize(self, input_title: str, top_k: int = 5, fuzzy_threshold: int = 80) -> List[Dict]:\n",
    "        \"\"\"Normalize a job title and return similar matches\"\"\"\n",
    "        \n",
    "        # 1. Exact match\n",
    "        if input_title.lower() in self.job_graph.title_to_id:\n",
    "            job_id = self.job_graph.title_to_id[input_title.lower()]\n",
    "            job = self.job_graph.job_lookup[job_id]\n",
    "            return [{\n",
    "                \"title\": job.title,\n",
    "                \"job_id\": job_id,\n",
    "                \"level\": job.level,\n",
    "                \"family\": job.family,\n",
    "                \"soc_category\": job.soc_category,\n",
    "                \"similarity_score\": 1.0,\n",
    "                \"match_type\": \"exact\"\n",
    "            }]\n",
    "        \n",
    "        # 2. Fuzzy matching\n",
    "        fuzzy_matches = process.extract(\n",
    "            input_title, \n",
    "            self.all_titles, \n",
    "            scorer=fuzz.token_sort_ratio,\n",
    "            limit=top_k * 2\n",
    "        )\n",
    "        \n",
    "        fuzzy_results = []\n",
    "        for match_title, score, _ in fuzzy_matches:\n",
    "            if score >= fuzzy_threshold:\n",
    "                job = self.title_to_job[match_title]\n",
    "                fuzzy_results.append({\n",
    "                    \"title\": job.title,\n",
    "                    \"job_id\": job.id,\n",
    "                    \"level\": job.level,\n",
    "                    \"family\": job.family,\n",
    "                    \"soc_category\": job.soc_category,\n",
    "                    \"similarity_score\": score / 100.0,\n",
    "                    \"match_type\": \"fuzzy\"\n",
    "                })\n",
    "        \n",
    "        # 3. Semantic similarity\n",
    "        input_embedding = self.model.encode([input_title])\n",
    "        similarities = cosine_similarity(input_embedding, self.title_embeddings)[0]\n",
    "        \n",
    "        top_indices = np.argsort(similarities)[-top_k * 2:][::-1]\n",
    "        \n",
    "        semantic_results = []\n",
    "        for idx in top_indices:\n",
    "            match_title = self.all_titles[idx]\n",
    "            job = self.title_to_job[match_title]\n",
    "            semantic_results.append({\n",
    "                \"title\": job.title,\n",
    "                \"job_id\": job.id,\n",
    "                \"level\": job.level,\n",
    "                \"family\": job.family,\n",
    "                \"soc_category\": job.soc_category,\n",
    "                \"similarity_score\": float(similarities[idx]),\n",
    "                \"match_type\": \"semantic\"\n",
    "            })\n",
    "        \n",
    "        # Combine and deduplicate\n",
    "        seen_ids = set()\n",
    "        combined_results = []\n",
    "        \n",
    "        for result_list in [fuzzy_results, semantic_results]:\n",
    "            for result in result_list:\n",
    "                if result[\"job_id\"] not in seen_ids:\n",
    "                    seen_ids.add(result[\"job_id\"])\n",
    "                    combined_results.append(result)\n",
    "        \n",
    "        # Sort by similarity\n",
    "        combined_results.sort(key=lambda x: x[\"similarity_score\"], reverse=True)\n",
    "        \n",
    "        return combined_results[:top_k]\n",
    "    \n",
    "    def save(self, filepath: str):\n",
    "        \"\"\"Save normalizer data\"\"\"\n",
    "        print(f\"Saving normalizer to {filepath}...\")\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'all_titles': self.all_titles,\n",
    "                'title_to_job': {k: asdict(v) for k, v in self.title_to_job.items()},\n",
    "                'title_embeddings': self.title_embeddings,\n",
    "            }, f)\n",
    "        print(\"Normalizer saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build normalizer\n",
    "normalizer = JobTitleNormalizer(job_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test normalization\n",
    "test_titles = [\n",
    "    \"Software Developer\",\n",
    "    \"ML Engineer\",\n",
    "    \"Product Lead\",\n",
    "    \"VP of Engineering\",\n",
    "    \"Data Analyst\",\n",
    "    \"UX Designer\",\n",
    "    \"Sales Rep\",\n",
    "]\n",
    "\n",
    "for test_title in test_titles:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Input: '{test_title}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "    results = normalizer.normalize(test_title, top_k=5)\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"{i}. {result['title']}\")\n",
    "        print(f\"   Score: {result['similarity_score']:.3f} | Level: {result['level']} | \"\n",
    "              f\"Family: {result['family']} | Type: {result['match_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"/home/claude/job_architecture_data\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Saving all data...\")\n",
    "\n",
    "# Save job graph\n",
    "job_graph.save(str(output_dir / \"job_graph.json\"))\n",
    "\n",
    "# Save normalizer\n",
    "normalizer.save(str(output_dir / \"normalizer_data.pkl\"))\n",
    "\n",
    "# Save statistics\n",
    "stats = {\n",
    "    \"total_jobs\": len(job_titles),\n",
    "    \"total_searchable_titles\": len(normalizer.all_titles),\n",
    "    \"families\": soc_df['family'].value_counts().to_dict(),\n",
    "    \"levels\": soc_df['level'].value_counts().to_dict(),\n",
    "    \"soc_categories\": int(soc_df['soc5_title'].nunique())\n",
    "}\n",
    "\n",
    "with open(output_dir / \"statistics.json\", 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(f\"\\nAll data saved to {output_dir}\")\n",
    "print(f\"\\nStatistics:\")\n",
    "for key, value in stats.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"  {key}: {len(value)} unique values\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export sample of processed data\n",
    "sample_df = soc_df.sample(min(1000, len(soc_df)))\n",
    "sample_df.to_csv(output_dir / \"sample_processed_titles.csv\", index=False)\n",
    "\n",
    "print(f\"Saved sample of {len(sample_df)} titles to sample_processed_titles.csv\")\n",
    "\n",
    "# Create level/family summary\n",
    "summary = soc_df.groupby(['family', 'level']).size().reset_index(name='count')\n",
    "summary = summary.pivot(index='family', columns='level', values='count').fillna(0).astype(int)\n",
    "summary.to_csv(output_dir / \"family_level_summary.csv\")\n",
    "\n",
    "print(\"\\nFamily x Level Distribution:\")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"JOB ARCHITECTURE SYSTEM - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n✅ Processed {len(soc_df):,} job titles from SOC dataset\")\n",
    "print(f\"✅ Created {len(job_titles):,} unique normalized job titles\")\n",
    "print(f\"✅ Built graph with {len(job_graph.graph.nodes):,} nodes and {len(job_graph.graph.edges):,} edges\")\n",
    "print(f\"✅ Generated embeddings for {len(normalizer.all_titles):,} searchable titles\")\n",
    "print(f\"✅ Classified into {len(stats['families'])} job families\")\n",
    "print(f\"✅ Organized into {len(stats['levels'])} organizational levels\")\n",
    "print(f\"\\nJob Families: {', '.join(sorted(stats['families'].keys()))}\")\n",
    "print(f\"\\nOrganizational Levels: 0 (Intern) → 9 (C-Suite)\")\n",
    "print(f\"\\nData saved to: {output_dir}\")\n",
    "print(f\"\\nNext step: Run the web service creation cells\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
